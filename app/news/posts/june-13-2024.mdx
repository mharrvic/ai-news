---
title: "AI News"
publishedAt: "2024-06-13"
summary: "Hybrid SSM/Transformers Outperform Pure Models, Mixture-of-Agents (MoA) Boosts LLM Performance, Stable Diffusion 3 Released, LLM Training and Fine-Tuning Innovations, Advancements in Multimodal Models, Hugging Face Acquires Argilla, AI Reddit Community Reactions, OpenAI's Revenue Milestone, New LLM Benchmarks and Tools, Community-Driven AI Developments"
---

### RAG Systems

- No updates.

### Language Models

1. **Hybrid SSM/Transformers Outperform Pure Models**:
   Recent studies show that combining Mamba and Transformer blocks achieves better performance than using either model alone. This hybrid approach is more efficient in terms of training and inference costs, making it a superior choice for various AI applications. [Read more](https://arxiv.org/pdf/2406.07887).
2. **Mixture-of-Agents (MoA) Boosts LLM Performance**:
   The Mixture-of-Agents (MoA) architecture, which layers multiple LLMs, significantly improves generation quality, outperforming single LLM models like GPT-4 Omni on benchmarks such as AlpacaEval 2.0. [Learn more](https://twitter.com/bindureddy/status/1801010849160818701).
3. **Advancements in Multimodal Models**:
   Luma Labs' Dream Machine showcases impressive text-to-video capabilities, while Table-LLaVa enhances multimodal table understanding, outperforming many existing models on benchmarks. [Explore the models](https://twitter.com/karpathy/status/1801305852735115357).
4. **New LLM Benchmarks and Tools**:
   The introduction of LiveBench AI and other benchmarks aim to provide objective evaluations of LLM capabilities, focusing on reasoning, coding, writing, and data analysis. These benchmarks help in setting new standards for LLM performance. [Find out more](https://twitter.com/ylecun/status/1800897325759701489).

### Fine-tuning

5. **LLM Training and Fine-Tuning Innovations**:
   New methods in LLM training, such as Memory Tuning and evolutionary strategies for optimizing loss functions, show significant improvements in model accuracy and efficiency. These techniques are being adopted for complex tasks like SQL agent operations and preference optimization. [Discover more](https://twitter.com/realSharonZhou/status/1801271891954696317).

### Security

- No updates.

### Others

6. **Stable Diffusion 3 Released**:
   Stability AI has released Stable Diffusion 3, which includes enhancements in text encoding and a multimodal diffusion transformer. The model has received mixed feedback, with some users praising its capabilities and others pointing out issues with anatomical accuracy. [Details here](https://huggingface.co/stabilityai/stable-diffusion-3-medium).

7. **Hugging Face Acquires Argilla**:
   In a strategic move, Hugging Face has acquired Argilla, aiming to enhance dataset creation and open-source contributions, bolstering the AI community's collaborative efforts. [Read more](https://twitter.com/_philschmid/status/1801274502879273009).

8. **AI Reddit Community Reactions**:
   The release of Stable Diffusion 3 has sparked discussions on platforms like Reddit, highlighting its strengths and limitations. Users report mixed experiences with its performance on complex prompts and artistic styles. [Join the discussion](https://www.reddit.com/r/StableDiffusion/comments/1de2qne/announcing_the_open_release_of_stable_diffusion_3/).

9. **OpenAI's Revenue Milestone**:
   OpenAI's annual revenue has doubled, driven primarily by direct sales of products like ChatGPT. This growth reflects the increasing demand for advanced AI services and tools. [More information](https://www.theinformation.com/articles/openais-annualized-revenue-doubles-to-3-4-billion-since-late-2023).

10. **Community-Driven AI Developments**:
    Platforms like Discord and GitHub continue to foster AI innovation through community collaboration. Projects such as OpenHermes-2.5 and RizzCon-Answering-Machine demonstrate the power of open-source contributions in advancing AI technology. [Engage with the community](https://github.com/togethercomputer/moa).
